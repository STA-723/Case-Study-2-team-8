---
title: "Untitled"
author: "Rihui Ou"
date: "1/23/2020"
output: html_document
---

```{r setup, include=FALSE}
library(dplyr)
library(tibble)
library(ggplot2)
library(plotly)
library(wordcloud) #word
library(tm) #word
library(MBA)
library(fields)
library(spBayes) #spatial modeling
set.seed(1927)
```

## Date Cleaning
```{r}
abnb <- read.csv("AB_NYC_2019.csv") %>%
        filter(price>0.001) #only consider the data with price>0.001
###Extract the coordiantes
cord=as.matrix(data %>% select(longitude,latitude))
## Drop $10,000 sites as they seem to be placeholders/spoofs
## (see the airbnb website and also the histogram of prices)
abnb <- abnb[abnb$price != 1e4,]

## Notice Reviews per Month is set to 0 for exactly those sites that
## have no reviews
sum(is.na(abnb$reviews_per_month) != (abnb$number_of_reviews == 0)) 
## Set those reviews per month to 0
abnb$reviews_per_month[is.na(abnb$reviews_per_month)] <- 0
## Drop sites with no availability
abnb <- abnb[abnb$availability_365 != 0,]

## Calculate Popularity Proxy
abnb$popularity <- abnb$reviews_per_month*12/abnb$availability_365

## Popularity should now be a measure of reviews/day available
## If no sites have dramatically changed their availability (i.e. significantly decreased it) these popularity scores should fall between 0 and 1
## Drop sites with popularity over 1 since they violate our 'constant-availability' assumption


## Add name-based features
names <- abnb$name
names <- as.character(names)
names <- strsplit(tolower(names), " ")

words <- unlist(names)
# uncomment next line to see word list
#summary(factor(words))

## Somewhat ad hoc, selecting among most common words from complete list of names. Trying to select words that don't indicate location or type of listing (e.g. "Midtown", "House"), and grouping words by intuition. Given unlimited time could try some kind of latent topic modeling for principled grouping
adjectives <- list(c("cozy", "comfy", "comfortable", "charm", "charming", "quiet"), c("spacious", "large", "huge", "big", "space"), c("beautiful", "lovely", "gorgeous", "view"), c("new", "bright", "clean", "sunny", "modern"), c("great", "amazing","prime", "best", "perfect"))

features <- matrix(data = 0, nrow = length(names), ncol = length(adjectives))

for(n in 1:length(names)){
  for(c in  1:length(adjectives)){
    for(a in adjectives[[c]]){
      if(a %in% names[[n]]){features[n,c] <-  1}
    }
  }
}

abnb$comfort <- features[,1]
abnb$space <- features[,2]
abnb$beauty <- features[,3]
abnb$upkeep <- features[,4]
abnb$superlative <- features[,5]

cord=as.matrix(abnb %>% select(longitude,latitude))
#extract locations

summary(abnb)
```

```{r}
data=read.csv(file = 'AB_NYC_2019.csv') %>%
     na.omit() %>% #deleta all NAs %>% 
     filter(price>0.001) #only consider the data with price>0.001
cord=as.matrix(data %>% select(longitude,latitude))
#extract locations
```

Figure out the popular adjective words in room names.
```{r}
data$name = data$name %>% 
        tolower() ##convert it to lower case

namedoc=data$name %>% 
        paste( collapse = '') %>%
        strsplit(" ") 

word_table=table(namedoc) %>% sort(decreasing = TRUE) ##Print word frequency table

data=data %>% mutate(luxury=grepl('luxury',data$name) %>% as.numeric() )  %>% 
     mutate(spacious=grepl('spacious',data$name) %>% as.numeric() ) %>% 
     mutate(private=grepl('private',data$name ) %>% as.numeric() ) ####Create word indicators of these three words

```
The top 3 popular adjective words are: private, spacious, luxury.

##EDA
```{r}

```









## Model Building-Univariate
Build an univariate gaussian spatial regression model: 
```{r echo = T, results = 'hide'}
n.samples <- 5000
starting <- list("tau.sq" = 1, "sigma.sq" = 1, "phi" = 6)
tuning <- list("tau.sq" = 0.002, "sigma.sq" = 0.002, "phi" = 0.02)
priors <- list("beta.Flat", "tau.sq.IG" = c(2, 1),
    "sigma.sq.IG" = c(2, 1), "phi.Unif" = c(3, 30))
m.uni_pri <- spLM(log(price)~room_type+minimum_nights+luxury+private+spacious+calculated_host_listings_count+availability_365,
              data[1;10000,],
              cord[1:],
              knots = c(5, 5, 0),
              starting = starting,
              tuning = tuning, 
              priors = priors, 
              cov.model = "exponential",
              n.samples = n.samples)
```


Try to summarize the posterior samples
```{r echo = T, results = 'hide'}
m.uni_pri <- spRecover(m.uni_pri, start = 1000)
```
Do posterior summary: room_type, minimum_night, name "luxury" , host_listing_number are significant.
```{r}
summary_pri=m.uni_pri$p.beta.samples %>% summary(quantile=c(.025,.975))
summary_pri=cbind(summary_pri$statistics[,1],summary_pri$quantiles) %>% round(4)
colnames(summary_pri)= c("mean", "2.5%", "97.5%")
summary_pri
```

Do the contour plot of $w$ using 500 datapoints.-not very meaningful
```{r}
res=3000
w.hat_pri <- apply(m.uni_pri$p.w.recover.samples, 1, median)
w.hat_pri_quant <- apply(m.uni_pri$p.w.recover.samples, 1, quantile, probs = c(0.05, 0.95),  na.rm = TRUE)
price_contour=data.frame(x=cord[1:res,1],
                         y=cord[1:res,2],
                         z=w.hat[1:res]) 
w.hat.surf<-mba.surf(cbind(cord, w.hat_pri), no.X = res, no.Y = res,extend = TRUE)$xyz.est
plot_ly(x=w.hat.surf$x,
        y=w.hat.surf$y,
    z=w.hat.surf$z, 
    type="contour",
    contours = list(coloring = 'heatmap')
    )

```
Look at the mean for each borough or neighbourhood: Manhattan is the most expensive borough. Tribeca, Chelsea,  West Village are top three most expensive neighbour_hoods.
```{r}
borough_names=unique(data$neighbourhood_group)
####Ranking Boroughs
borough_table_pri=rep(0,length(borough_names))
names(borough_table_pri)<-borough_names
i=1
for (name in borough_names) {
        borough_table_pri[i]=w.hat_pri[which(data$neighbourhood_group==name)] %>% mean()
        i=i+1
}


####Ranking Neighbourhood
neigh_names=unique(data$neighbourhood)
neigh_table_pri=rep(0,length(neigh_names))
names(neigh_table_pri)<-neigh_names
i=1
for (name in neigh_names) {
        neigh_table_pri[i]=w.hat_pri[which(data$neighbourhood==name)] %>% mean()
        i=i+1
}


```

```{r}
borough_table_pri  %>% sort
neigh_table_pri %>% sort() ###Top 5
```
Look at the quantiles of boroughs
```{r}

borough_table_pri_quant=matrix(0,5,2)
rownames(borough_table_pri_quant)<-borough_names
i=1
for (name in borough_names) {
        borough_table_pri_quant[i,]=w.hat_pri_quant[,which(data$neighbourhood_group==name)] %>% rowMeans()
        i=i+1
}
borough_tab=cbind(borough_table_pri_quant,borough_table_pri) 
colnames(borough_tab)<-c("5%","95%","mean")
borough_tab[order(borough_tab[,3]),]
```
Look at the quantiles of neighbourhoods

Do some plot
```{r}
set.seed(1911)
a=m.uni_pri$p.w.recover.samples
wvalue_matrix=NULL
for (name in borough_names) {
       indices=which(data$neighbourhood_group==name)
       subind=sample(indices,30)
       wvalue=a[subind,sample(4000,30)] %>% as.vector() %>% round(2)
       group=rep(name,900)
       wvalue_matrix=rbind(wvalue_matrix,cbind(wvalue,group))
}
wvalue_matrix=as.data.frame(wvalue_matrix)
wvalue_matrix$group=wvalue_matrix$group %>% as.factor()
ggplot() + 
geom_density(data=wvalue_matrix, aes(x=wvalue, group=group, fill=group),alpha=0.4)
```


## Build a model to model popularity
Similarly a spatial regression model is built. This time we study the popularity instaed.
```{r echo = T, results = 'hide'}
n.samples <- 5000
starting <- list("tau.sq" = 1, "sigma.sq" = 1, "phi" = 6)
tuning <- list("tau.sq" = 0.002, "sigma.sq" = 0.002, "phi" = 0.02)
priors <- list("beta.Flat", "tau.sq.IG" = c(2, 1),
    "sigma.sq.IG" = c(2, 1), "phi.Unif" = c(3, 30))
m.uni_pop <- spLM(log(reviews_per_month)~room_type+minimum_nights+luxury+private+spacious+calculated_host_listings_count+availability_365,
              data[1:5000,],
              cord[1:5000,],
              knots = c(5, 5, 0),
              starting = starting,
              tuning = tuning, 
              priors = priors, 
              cov.model = "exponential",
              n.samples = n.samples)
```


Try to summarize the posterior samples
```{r echo = T, results = 'hide'}
m.uni_pop <- spRecover(m.uni_pop, start = 1000)
```

```{r}
m.uni_pop$p.beta.samples %>% summary(quantile=c(.025,.975))
```

Look at the mean for each borough or neighbourhood: Manhattan is the most expensive borough. Tribeca, Chelsea,  West Village are top three most expensive neighbour_hoods.
```{r}

w.hat <- apply(m.uni_pop$p.w.recover.samples, 1, median) ###Compute predictive w
borough_names=unique(data$neighbourhood_group)
####Ranking Boroughs
borough_table=rep(0,length(borough_names))
names(borough_table)<-borough_names
i=1
for (name in borough_names) {
        borough_table[i]=w.hat[which(data[1:5000,]$neighbourhood_group==name)] %>% mean()
        i=i+1
}
borough_table %>% sort()

```


## Multivariate Modeling (Optional)
```{r}
q=2
A.starting <- diag(1,q)[lower.tri(diag(1,q), TRUE)]
starting <- list("phi"=rep(3/0.5,q), "A"=A.starting, "Psi"=rep(1,q))
tuning <- list("phi"=rep(0.01,q), "A"=rep(0.001,length(A.starting)), "Psi"=rep(0.0001,q))
priors <- list("beta.Flat", "phi.Unif"=list(rep(3/0.75,q), rep(3/0.25,q)),
               "K.IW"=list(q+1, diag(0.1,q)), "Psi.ig"=list(c(2,2), c(0.1,0.1)))
m=spMvLM(list(log(price)~room_type+minimum_nights,
            log(reviews_per_month)~room_type+minimum_nights),
       data[1:10000,],
       cord[1:10000,],
       knots = c(3, 3, 0),
       cov.model = "exponential",
       n.samples = 3000,
       priors=priors,
       starting = starting,
       tuning=tuning,
       verbose=TRUE)
betaw_samps=m %>% spRecover() #obtain beta & w samples
betaw_samps$p.beta.recover.samples %>% summary()
```


